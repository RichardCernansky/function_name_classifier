@article{code2vec,
author = {Alon, Uri and Zilberstein, Meital and Levy, Omer and Yahav, Eran},
title = {code2vec: learning distributed representations of code},
year = {2019},
issue_date = {January 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {3},
number = {POPL},
url = {https://doi.org/10.1145/3290353},
doi = {10.1145/3290353},
abstract = {We present a neural model for representing snippets of code as continuous distributed vectors (``code embeddings''). The main idea is to represent a code snippet as a single fixed-length code vector, which can be used to predict semantic properties of the snippet. To this end, code is first decomposed to a collection of paths in its abstract syntax tree. Then, the network learns the atomic representation of each path while simultaneously learning how to aggregate a set of them.  We demonstrate the effectiveness of our approach by using it to predict a method's name from the vector representation of its body. We evaluate our approach by training a model on a dataset of 12M methods. We show that code vectors trained on this dataset can predict method names from files that were unobserved during training. Furthermore, we show that our model learns useful method name vectors that capture semantic similarities, combinations, and analogies.  A comparison of our approach to previous techniques over the same dataset shows an improvement of more than 75\%, making it the first to successfully predict method names based on a large, cross-project corpus. Our trained model, visualizations and vector similarities are available as an interactive online demo at http://code2vec.org. The code, data and trained models are available at https://github.com/tech-srl/code2vec.},
journal = {Proc. ACM Program. Lang.},
month = jan,
articleno = {40},
numpages = {29},
keywords = {Big Code, Distributed Representations, Machine Learning}
}

@article{samoaa,
  author={H. P. Samoaa and F. Bayram and P. Salza and P. Leitner},
  title={A Systematic Mapping Study of Source Code Representation for Deep Learning in Software Engineering},
  journal={Software: Practice and Experience},
  year={2022},
  month={June},
  doi={10.1049/sfw2.12064},
  url={https://doi.org/10.1049/sfw2.12064}
}

@misc{sun2023AST,
      title={Abstract Syntax Tree for Programming Language Understanding and Representation: How Far Are We?}, 
      author={Weisong Sun and Chunrong Fang and Yun Miao and Yudu You and Mengzhe Yuan and Yuchen Chen and Quanjun Zhang and An Guo and Xiang Chen and Yang Liu and Zhenyu Chen},
      year={2023},
      eprint={2312.00413},
      archivePrefix={arXiv},
      primaryClass={cs.SE},
      url={https://arxiv.org/abs/2312.00413}, 
}

@article{path-based-representation,
author = {Alon, Uri and Zilberstein, Meital and Levy, Omer and Yahav, Eran},
title = {A general path-based representation for predicting program properties},
year = {2018},
issue_date = {April 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {53},
number = {4},
issn = {0362-1340},
url = {https://doi.org/10.1145/3296979.3192412},
doi = {10.1145/3296979.3192412},
abstract = {Predicting program properties such as names or expression types has a wide range of applications. It can ease the task of programming, and increase programmer productivity. A major challenge when learning from programs is how to represent programs in a way that facilitates effective learning. We present a general path-based representation for learning from programs. Our representation is purely syntactic and extracted automatically. The main idea is to represent a program using paths in its abstract syntax tree (AST). This allows a learning model to leverage the structured nature of code rather than treating it as a flat sequence of tokens. We show that this representation is general and can: (i) cover different prediction tasks, (ii) drive different learning algorithms (for both generative and discriminative models), and (iii) work across different programming languages. We evaluate our approach on the tasks of predicting variable names, method names, and full types. We use our representation to drive both CRF-based and word2vec-based learning, for programs of four languages: JavaScript, Java, Python and C#. Our evaluation shows that our approach obtains better results than task-specific handcrafted representations across different tasks and programming languages.},
journal = {SIGPLAN Not.},
month = jun,
pages = {404–419},
numpages = {16},
keywords = {Programming Languages, Machine Learning, Learning Representations, Big Code}
}

@inproceedings{alon,
author = {Alon, Uri and Zilberstein, Meital and Levy, Omer and Yahav, Eran},
title = {A general path-based representation for predicting program properties},
year = {2018},
isbn = {9781450356985},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3192366.3192412},
doi = {10.1145/3192366.3192412},
abstract = {Predicting program properties such as names or expression types has a wide range of applications. It can ease the task of programming, and increase programmer productivity. A major challenge when learning from programs is how to represent programs in a way that facilitates effective learning. We present a general path-based representation for learning from programs. Our representation is purely syntactic and extracted automatically. The main idea is to represent a program using paths in its abstract syntax tree (AST). This allows a learning model to leverage the structured nature of code rather than treating it as a flat sequence of tokens. We show that this representation is general and can: (i) cover different prediction tasks, (ii) drive different learning algorithms (for both generative and discriminative models), and (iii) work across different programming languages. We evaluate our approach on the tasks of predicting variable names, method names, and full types. We use our representation to drive both CRF-based and word2vec-based learning, for programs of four languages: JavaScript, Java, Python and C#. Our evaluation shows that our approach obtains better results than task-specific handcrafted representations across different tasks and programming languages.},
booktitle = {Proceedings of the 39th ACM SIGPLAN Conference on Programming Language Design and Implementation},
pages = {404–419},
numpages = {16},
keywords = {Programming Languages, Machine Learning, Learning Representations, Big Code},
location = {Philadelphia, PA, USA},
series = {PLDI 2018}
}

@misc{bogomolov,
      title={Authorship Attribution of Source Code: A Language-Agnostic Approach and Applicability in Software Engineering}, 
      author={Egor Bogomolov and Vladimir Kovalenko and Yurii Rebryk and Alberto Bacchelli and Timofey Bryksin},
      year={2021},
      eprint={2001.11593},
      archivePrefix={arXiv},
      primaryClass={cs.SE},
      url={https://arxiv.org/abs/2001.11593}, 
}

@ARTICLE{attention,
  author={Galassi, Andrea and Lippi, Marco and Torroni, Paolo},
  journal={IEEE Transactions on Neural Networks and Learning Systems}, 
  title={Attention in Natural Language Processing}, 
  year={2021},
  volume={32},
  number={10},
  pages={4291-4308},
  keywords={Task analysis;Computer architecture;Visualization;Neural networks;Natural language processing;Taxonomy;Computational modeling;Natural language processing (NLP);neural attention;neural networks;review;survey},
  doi={10.1109/TNNLS.2020.3019893}}

@inproceedings{DanielWatson,
  title={Source Code Stylometry and Authorship Attribution for Open Source},
  author={Daniel Watson},
  year={2019},
  url={https://api.semanticscholar.org/CorpusID:208070402}
}