{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a4876df-d80c-4640-a3fc-cd273f8034e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional\n",
    "import ndjson\n",
    "import numpy as np\n",
    "import pdb\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import pickle\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras \n",
    "from keras.models import load_model \n",
    "from keras.saving import custom_object_scope\n",
    "from keras.layers import Input, Embedding, Dense, Activation, Lambda, Concatenate, Softmax, Layer\n",
    "from keras.models import Model\n",
    "from keras.callbacks import LambdaCallback\n",
    "from keras.utils import pad_sequences,plot_model\n",
    "from keras.activations import softmax\n",
    "\n",
    "from collections import OrderedDict #for ordered sets of the data\n",
    "\n",
    "class Node:\n",
    "    def __init__(self, b_i: Optional[int], kind: str, code_pos: str, data: str):\n",
    "        self.branching_idx = b_i\n",
    "        self.parent = None\n",
    "        self.children = []\n",
    "        self.kind = kind\n",
    "        self.code_pos = code_pos\n",
    "        self.data = data\n",
    "\n",
    "    def set_parent(self, parent: 'Node'):\n",
    "        self.parent = parent\n",
    "\n",
    "    def add_child(self, child: 'Node'):\n",
    "        self.children.append(child)\n",
    "\n",
    "    def to_dict(self):\n",
    "        \"\"\"Convert the node and its children to a dictionary.\"\"\"\n",
    "        return {\n",
    "            'kind': self.kind,\n",
    "            'code_pos': self.code_pos,\n",
    "            'data': self.data,\n",
    "            'children': [child.to_dict() for child in self.children]\n",
    "        }\n",
    "\n",
    "def json_to_tree(data: dict) -> Node:\n",
    "    \"\"\"\n",
    "    Recursively builds a tree of Node objects from a JSON dictionary.\n",
    "    \"\"\"\n",
    "    node = Node(\n",
    "        b_i=None,\n",
    "        kind=data.get('kind'),\n",
    "        code_pos=data.get('code_pos'),\n",
    "        data=data.get('data')\n",
    "    )\n",
    "\n",
    "    # Recursively add children\n",
    "    for child_data in data.get('children', []):\n",
    "        child_node = json_to_tree(child_data)\n",
    "        child_node.set_parent(node)  # Set the parent for the child node\n",
    "        node.add_child(child_node)\n",
    "\n",
    "    return node\n",
    "\n",
    "#NODE TO NODE PATHS\n",
    "# Function to collect all leaf nodes iteratively using DFS\n",
    "def collect_leaves_iterative(root):\n",
    "    if root is None:\n",
    "        return []\n",
    "\n",
    "    stack = [(root, [])]  # Stack to store (node, path_from_root)\n",
    "    leaves = []  # List to store leaf nodes and their paths\n",
    "\n",
    "    while stack:\n",
    "        node, path = stack.pop()\n",
    "        current_path = path + [node.kind]  # Update the current path\n",
    "\n",
    "        # leaf node - has no children\n",
    "        if not node.children:\n",
    "            leaves.append((node, current_path))\n",
    "\n",
    "        # push the children to the stack for DFS\n",
    "        children = reversed(node.children)\n",
    "        for child in children:  # process children in order on the stack\n",
    "            stack.append((child, current_path))\n",
    "\n",
    "    return leaves\n",
    "\n",
    "\n",
    "# Function to find the Lowest Common Ancestor (LCA) iteratively\n",
    "def find_lca_iterative(n1_path, n2_path):\n",
    "    length = len(n1_path) if len(n1_path) < len(n2_path) else len(n2_path)\n",
    "\n",
    "    lca = None\n",
    "    for i in range(length):\n",
    "        if n1_path[i] == n2_path[i]:\n",
    "            lca = n1_path[i]\n",
    "        else:\n",
    "            break\n",
    "    return lca\n",
    "\n",
    "\n",
    "def find_leaf_to_leaf_paths_iterative(root):\n",
    "    leaf_nodes = collect_leaves_iterative(root)\n",
    "\n",
    "    #list of all leaf-to-leaf paths\n",
    "    leaf_to_leaf_paths = []\n",
    "\n",
    "    # Iterate over each pair of leaf nodes\n",
    "    for i in range(len(leaf_nodes)):\n",
    "        for j in range(i + 1, len(leaf_nodes)):\n",
    "            leaf1, path1 = leaf_nodes[i]\n",
    "            leaf2, path2 = leaf_nodes[j]\n",
    "\n",
    "            # find lca\n",
    "            lca = find_lca_iterative(path1, path2)\n",
    "\n",
    "            # find the indexes\n",
    "            lca_index1 = path1.index(lca)\n",
    "            lca_index2 = path2.index(lca)\n",
    "\n",
    "            # Path from leaf1 to leaf2 via the LCA\n",
    "            path_to_lca_from_leaf1 = path1[:lca_index1 + 1]\n",
    "            path_to_lca_from_leaf2 = path2[:lca_index2 + 1]\n",
    "            path_to_lca_from_leaf2.reverse()\n",
    "\n",
    "            #combine the paths\n",
    "            complete_path = path_to_lca_from_leaf1 + path_to_lca_from_leaf2[1:]\n",
    "\n",
    "            # Add the complete leaf-to-leaf path to the result\n",
    "            leaf_to_leaf_paths.append((leaf1.data,)+tuple(complete_path)+(leaf2.data,))\n",
    "\n",
    "\n",
    "    return [node.data for node,path in leaf_nodes], leaf_to_leaf_paths\n",
    "\n",
    "def find_tag(root) -> str:\n",
    "    # root is FunctionDefinition\n",
    "    definition_node = root\n",
    "    for definition_child in definition_node.children:\n",
    "        if definition_child.kind == \"FunctionDeclarator\":\n",
    "            declarator_node = definition_child\n",
    "            for declarator_child in declarator_node.children:\n",
    "                if declarator_child.kind == \"IdentifierDeclarator\":\n",
    "                    return str(declarator_child.data)\n",
    "\n",
    "            \n",
    "class WeightedContextLayer(Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        super(WeightedContextLayer, self).__init__(**kwargs)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        attention_weights, transformed_contexts = inputs\n",
    "        # Compute the weighted context\n",
    "        weighted_context = tf.reduce_sum(attention_weights * transformed_contexts, axis=1)\n",
    "        return weighted_context\n",
    "\n",
    "\n",
    "class TagEmbeddingMatrixLayer(Layer):\n",
    "    def __init__(self, tags_vocab_size, embedding_dim, **kwargs):\n",
    "        super(TagEmbeddingMatrixLayer, self).__init__(**kwargs)\n",
    "        self.tags_vocab_size = tags_vocab_size\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.tag_embedding = None  # Initialize here\n",
    "        self.tag_embedding = Embedding(input_dim=self.tags_vocab_size, \n",
    "                                       output_dim=self.embedding_dim, \n",
    "                                       name='tag_embedding', \n",
    "                                       mask_zero=True)\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        # transpose the tag embeddings\n",
    "        tags_embedding_matrix = self.tag_embedding(tf.range(self.tag_embedding.input_dim))  # Shape: (tags_vocab_size, embedding_dim)\n",
    "        tags_embedding_matrix_t = tf.transpose(tags_embedding_matrix)  # Shape: (embedding_dim, tags_vocab_size)\n",
    "        \n",
    "        # num_repeats based on the shape of weighted_context\n",
    "        num_repeats = tf.math.ceil( (tf.shape(inputs)[1] / tf.shape(tags_embedding_matrix_t)[0]))\n",
    "        num_repeats = tf.cast(num_repeats, tf.int32)  # Ensure it's an integer\n",
    "\n",
    "        # # tile it\n",
    "        tags_embedding_matrix_t_tiled = tf.tile(tags_embedding_matrix_t, [num_repeats, 1])  # Shape: (num_repeats * embedding_dim, tags_vocab_size)\n",
    "        \n",
    "       \n",
    "        \n",
    "        # # only the required portion         # Shape: (weighted_context.shape[1], tags_vocab_size\n",
    "        matrix_final  = tf.matmul( inputs, tags_embedding_matrix_t_tiled[:(tf.shape(inputs)[1])] )\n",
    "        \n",
    "        return matrix_final\n",
    "\n",
    "def softmaxAxis1(x):\n",
    "    return softmax(x,axis=1)\n",
    "\n",
    "\n",
    "def get_vocabs(vocabs_pkl):\n",
    "    with open(vocabs_pkl, 'rb') as f:\n",
    "        vocabs = pickle.load(f)\n",
    "        return vocabs['value_vocab'], vocabs['path_vocab'], vocabs['tags_vocab'], vocabs['max_num_contexts']\n",
    "\n",
    "def preprocess_function(function_json, value_vocab, path_vocab, tags_vocab, max_num_contexts):\n",
    "    func_root = json_to_tree(function_json)\n",
    "    func_values, func_paths = find_leaf_to_leaf_paths_iterative(func_root)\n",
    "    tag = find_tag(func_root)\n",
    "    \n",
    "    func_root = json_to_tree(function_json)\n",
    "    tag = find_tag(func_root)\n",
    "    _, func_paths = find_leaf_to_leaf_paths_iterative(func_root) # get all contexts\n",
    "\n",
    "    sts_indices = []     # start terminals indices\n",
    "    paths_indices = []   # path indices\n",
    "    ets_indices = []     # end terminals indices\n",
    "    tag_idx = tags_vocab[tag]  # get the tag value\n",
    "    \n",
    "    for path in func_paths:  # map to the indices\n",
    "        sts_indices.append(value_vocab[path[0]])    # get the terminal node's data\n",
    "        paths_indices.append(path_vocab[path[1:-1]]) # get the path nodes' kinds\n",
    "        ets_indices.append(value_vocab[path[-1]])   # get the ending terminal node's data\n",
    "\n",
    "    sts_indices = pad_sequences([sts_indices], maxlen=max_num_contexts, padding='post', value=0)\n",
    "    paths_indices = pad_sequences([paths_indices], maxlen=max_num_contexts, padding='post', value=0)\n",
    "    ets_indices = pad_sequences([ets_indices], maxlen=max_num_contexts, padding='post', value=0)\n",
    "\n",
    "    return tag_idx, sts_indices, paths_indices, ets_indices\n",
    "\n",
    "\n",
    "vocabs_pkl = 'vocabs.pkl'\n",
    "test_file = 'strat_test_functionsASTs.ndjson'\n",
    "model_file = 'NEDELA_func_classifier_model.h5'\n",
    "\n",
    "value_vocab, path_vocab, tags_vocab, max_num_contexts = get_vocabs(vocabs_pkl)\n",
    "\n",
    "reverse_value_vocab = {idx: value for value, idx in value_vocab.items()}\n",
    "reverse_path_vocab = {idx: path for path, idx in path_vocab.items()}\n",
    "reverse_tags_vocab = {idx: tag for tag, idx in tags_vocab.items()}\n",
    "\n",
    "custom_objects = {\n",
    "    'softmaxAxis1': softmaxAxis1,\n",
    "    'WeightedContextLayer': WeightedContextLayer,\n",
    "    'TagEmbeddingMatrixLayer': TagEmbeddingMatrixLayer\n",
    "}\n",
    "with custom_object_scope(custom_objects):\n",
    "    model = load_model(model_file)\n",
    "\n",
    "total_processed = 0\n",
    "successful_processed = 0\n",
    "right_assigned = 0\n",
    "\n",
    "with open(test_file, 'r') as f:\n",
    "    data = ndjson.load(f)\n",
    "    \n",
    "    for line in data:\n",
    "        total_processed += 1 \n",
    "\n",
    "        try:\n",
    "            tag_idx, sts_indices, value_indices, ets_indices = preprocess_function(line, value_vocab, path_vocab, tags_vocab, max_num_contexts)\n",
    "            print(tag_idx)\n",
    "            \n",
    "            inputs = {\n",
    "                'value1_input': np.array(value_indices),\n",
    "                'path_input': np.array(sts_indices),\n",
    "                'value2_input': np.array(ets_indices)\n",
    "            }\n",
    "            \n",
    "            prediction = model.predict(inputs)\n",
    "\t\t            \n",
    "           \n",
    "            predicted_function_tag = reverse_tags_vocab[np.argmax(prediction)]  # Using argmax for softmax output\n",
    "            actual_tag = reverse_tags_vocab[tag_idx]\n",
    "\n",
    "            if predicted_function_tag == actual_tag:\n",
    "                right_assigned += 1 \n",
    "            \n",
    "            print(f\"Input function Tag: {actual_tag}. | Predicted function Tag: {predicted_function_tag}.\")\n",
    "            \n",
    "            successful_processed += 1  # Increment the successful count if preprocessing is successful\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing line: {e}\")\n",
    "\n",
    "success_ratio = successful_processed / total_processed if total_processed > 0 else 0\n",
    "right_ratio = right_assigned / successful_processed if total_processed > 0 else 0\n",
    "print(f\"Successfully processed: {successful_processed}/{total_processed} ({success_ratio * 100:.2f}%)\")\n",
    "print(f\"Right assigned from successfully processed: {right_assigned}/{successful_processed} ({right_ratio * 100:.2f}%)\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
