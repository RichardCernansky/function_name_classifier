{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cd2c909b3c341b53",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-04T10:53:52.993966Z",
     "start_time": "2024-10-04T10:53:52.990527Z"
    }
   },
   "outputs": [],
   "source": [
    "from typing import Optional\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras \n",
    "from tensorflow.keras.layers import Input, Embedding, Dense, Dot, Activation, Lambda\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "from collections import OrderedDict #for ordered sets of the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f1223b71-ee0b-49d1-a72a-adf5c9b7ecf2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-04T11:24:50.995399Z",
     "start_time": "2024-10-04T11:24:50.985255Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Concatenate' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 41\u001b[0m\n\u001b[1;32m     38\u001b[0m embedded_value2 \u001b[38;5;241m=\u001b[39m value_embedding(input_value2)  \u001b[38;5;66;03m# Shape: (None, num_context, embedding_dim)\u001b[39;00m\n\u001b[1;32m     40\u001b[0m \u001b[38;5;66;03m# Concatenate along the last axis (for each context, value1, path, and value2 are concatenated)\u001b[39;00m\n\u001b[0;32m---> 41\u001b[0m embedded_concat \u001b[38;5;241m=\u001b[39m \u001b[43mConcatenate\u001b[49m(axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)([embedded_value1, embedded_path, embedded_value2])\n\u001b[1;32m     42\u001b[0m \u001b[38;5;66;03m# Shape: (None, num_context, 3 * embedding_dim)\u001b[39;00m\n\u001b[1;32m     43\u001b[0m \n\u001b[1;32m     44\u001b[0m \u001b[38;5;66;03m# Apply a dense transformation to each concatenated context (row-wise transformation)\u001b[39;00m\n\u001b[1;32m     45\u001b[0m transformed_contexts \u001b[38;5;241m=\u001b[39m Dense(units\u001b[38;5;241m=\u001b[39my, activation\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtanh\u001b[39m\u001b[38;5;124m'\u001b[39m)(embedded_concat)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'Concatenate' is not defined"
     ]
    }
   ],
   "source": [
    "#having functions.ndjson \n",
    "#create set of leaf_node_values\n",
    "#create set of all distinct paths\n",
    "#create set of all tags_vocab\n",
    "\n",
    "value_vocab = set() #set of all leaf values\n",
    "path_vocab = set() #set of all distinct paths\n",
    "tags_vocab = set() #set of all distinct function tags\n",
    "\n",
    "#vocab sizes and embedding dimensions\n",
    "value_vocab_size = len(value_vocab)\n",
    "path_vocab_size = len(path_vocab)\n",
    "tags_vocab_size = len(tags_vocab)\n",
    "y = tags_vocab_size\n",
    "embedding_dim = 128 \n",
    "\n",
    "\n",
    "# Dummy vocabulary sizes and dimensions\n",
    "value_vocab_size = 10000  # Example vocab size for value vocab\n",
    "path_vocab_size = 5000    # Example vocab size for path vocab\n",
    "tags_vocab_size = 1000    # Example tag vocab size\n",
    "embedding_dim = 128       # Embedding dimension\n",
    "num_context = 20          # Number of contexts for a single function\n",
    "\n",
    "# inputs for value1, path, and value2 (with num_context inputs per batch)\n",
    "input_value1 = Input(shape=(num_context,), name='value1_input')\n",
    "input_path = Input(shape=(num_context,), name='path_input')\n",
    "input_value2 = Input(shape=(num_context,), name='value2_input')\n",
    "\n",
    "# Embedding layers\n",
    "value_embedding = Embedding(input_dim=value_vocab_size, output_dim=embedding_dim, name='value_embedding')\n",
    "path_embedding = Embedding(input_dim=path_vocab_size, output_dim=embedding_dim, name='path_embedding')\n",
    "tag_embedding = Embedding(input_dim=tags_vocab_size, output_dim=embedding_dim, name='tag_embedding')\n",
    "\n",
    "# Embed the inputs\n",
    "embedded_value1 = value_embedding(input_value1)  # Shape: (None, num_context, embedding_dim)\n",
    "embedded_path = path_embedding(input_path)      # Shape: (None, num_context, embedding_dim)\n",
    "embedded_value2 = value_embedding(input_value2)  # Shape: (None, num_context, embedding_dim)\n",
    "\n",
    "# Concatenate along the last axis (for each context, value1, path, and value2 are concatenated)\n",
    "embedded_concat = Concatenate(axis=-1)([embedded_value1, embedded_path, embedded_value2])\n",
    "# Shape: (None, num_context, 3 * embedding_dim)\n",
    "\n",
    "# Apply a dense transformation to each concatenated context (row-wise transformation)\n",
    "transformed_contexts = Dense(units=y, activation='tanh')(embedded_concat)\n",
    "# Shape: (None, num_context, y)\n",
    "\n",
    "# Attention mechanism\n",
    "attention_weights = Dense(1, activation='softmax')(transformed_contexts)\n",
    "# Shape: (None, num_context, 1) - attention scores for each context\n",
    "\n",
    "# apply attention weights to get the weighted sum of contexts\n",
    "weighted_context = tf.reduce_sum(attention_weights * transformed_contexts, axis=1)\n",
    "# shape: (None, embedding_dim) - weighted sum across contexts\n",
    "\n",
    "# get the tag embeddings\n",
    "tags_embedding_matrix = tag_embedding(tf.range(tags_vocab_size))  # Shape: (tags_vocab_size, embedding_dim)\n",
    "\n",
    "# compute the dot product between the weighted context and all tag embeddings\n",
    "tag_scores = tf.matmul(weighted_context, tags_embedding_matrix, transpose_b=True)  # Shape: (None, tags_vocab_size)\n",
    "\n",
    "# apply softmax to get probabilities over all tags\n",
    "output = Softmax()(tag_scores)\n",
    "# Shape: (None, tags_vocab_size) - probabilities for each tag\n",
    "\n",
    "# Define the model\n",
    "model = Model(inputs=[input_value1, input_path, input_value2], outputs=output)\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Summary of the model\n",
    "model.summary()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "41fa5b84-1041-41db-a23d-3647f362f068",
   "metadata": {},
   "outputs": [],
   "source": [
    "#NODE TO NODE PATHS\n",
    "# Function to collect all leaf nodes iteratively using DFS\n",
    "def collect_leaves_iterative(root):\n",
    "    if root is None:\n",
    "        return []\n",
    "\n",
    "    stack = [(root, [])]  # Stack to store (node, path_from_root)\n",
    "    leaves = []  # List to store leaf nodes and their paths\n",
    "\n",
    "    while stack:\n",
    "        node, path = stack.pop()\n",
    "        current_path = path + [node.kind]  # Update the current path\n",
    "\n",
    "        # leaf node - has no children\n",
    "        if not node.children:\n",
    "            leaves.append((node, current_path))\n",
    "\n",
    "        # push the children to the stack for DFS\n",
    "        children = reversed(node.children)\n",
    "        for child in children:  # process children in order on the stack\n",
    "            stack.append((child, current_path))\n",
    "\n",
    "    return leaves\n",
    "\n",
    "\n",
    "# Function to find the Lowest Common Ancestor (LCA) iteratively\n",
    "def find_lca_iterative(n1_path, n2_path):\n",
    "    length = n1_path.length if n1_path.length < n2_path.length else n2_path.length\n",
    "\n",
    "    lca = None\n",
    "    for i in range(length):\n",
    "        if n1_path[i] == n2_path[i]:\n",
    "            lca = n1_path[i]\n",
    "        else:\n",
    "            break\n",
    "    return lca\n",
    "\n",
    "\n",
    "def find_leaf_to_leaf_paths_iterative(root):\n",
    "    leaf_nodes = collect_leaves_iterative(root)\n",
    "\n",
    "    #list of all leaf-to-leaf paths\n",
    "    leaf_to_leaf_paths = []\n",
    "\n",
    "    # Iterate over each pair of leaf nodes\n",
    "    for i in range(len(leaf_nodes)):\n",
    "        for j in range(i + 1, len(leaf_nodes)):\n",
    "            leaf1, path1 = leaf_nodes[i]\n",
    "            leaf2, path2 = leaf_nodes[j]\n",
    "\n",
    "            # find lca\n",
    "            lca = find_lca_iterative(path1, path2)\n",
    "\n",
    "            # find the indexes\n",
    "            lca_index1 = path1.index(lca)\n",
    "            lca_index2 = path2.index(lca)\n",
    "\n",
    "            # Path from leaf1 to leaf2 via the LCA\n",
    "            path_to_lca_from_leaf1 = path1[:lca_index1 + 1]\n",
    "            path_to_lca_from_leaf2 = path2[:lca_index2 + 1]\n",
    "            path_to_lca_from_leaf2.reverse().pop()\n",
    "\n",
    "            #combine the paths\n",
    "            complete_path = path_to_lca_from_leaf1 + path_to_lca_from_leaf2\n",
    "\n",
    "            # Add the complete leaf-to-leaf path to the result\n",
    "            leaf_to_leaf_paths.append(complete_path)\n",
    "\n",
    "    return leaf_to_leaf_paths\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "53b22f4b-2e73-436f-8973-a39494496b41",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_vocabs(file_path):\n",
    "    # Open the .ndjson file\n",
    "    with open(file_path, 'r') as ndjson_file:\n",
    "        # Load the file content\n",
    "        data = ndjson.load(ndjson_file)\n",
    "\n",
    "        value_vocab = set() #set of all leaf values\n",
    "        path_vocab = set() #set of all distinct paths\n",
    "        tags_vocab = set() #set of all distinct function tags\n",
    "        \n",
    "        for function_json in data:\n",
    "            # convert each line (function) to a tree\n",
    "            func_root = json_to_tree(function_json)\n",
    "            func_values, func_paths = find_leaf_to_leaf_paths_iterative(root)\n",
    "\n",
    "            # add to vocabs new values from calling find_leafs_to_leaves\n",
    "\n",
    "        return value_vocab, path_vocab, tags_vocab\n",
    "            \n",
    "            \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1a80abc1-6d98-4d3e-a668-6070d9a2e201",
   "metadata": {},
   "outputs": [],
   "source": [
    "def json_to_tree(data: dict) -> Node:\n",
    "    \"\"\"\n",
    "    Recursively builds a tree of Node objects from a JSON dictionary.\n",
    "    \"\"\"\n",
    "    node = Node(\n",
    "        b_i=None,\n",
    "        kind=data.get('kind'),\n",
    "        code_pos=data.get('code_pos'),\n",
    "        data=data.get('data')\n",
    "    )\n",
    "\n",
    "    # Recursively add children\n",
    "    for child_data in data.get('children', []):\n",
    "        child_node = json_to_tree(child_data)\n",
    "        child_node.set_parent(node)  # Set the parent for the child node\n",
    "        node.add_child(child_node)\n",
    "\n",
    "    return node\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c1e8309b-79b6-423a-8d90-7cbb4e784b8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Node:\n",
    "    def __init__(self, b_i: Optional[int], kind: str, code_pos: str, data: str):\n",
    "        self.branching_idx = b_i\n",
    "        self.parent = None\n",
    "        self.children = []\n",
    "        self.kind = kind\n",
    "        self.code_pos = code_pos\n",
    "        self.data = data\n",
    "\n",
    "    def set_parent(self, parent: 'Node'):\n",
    "        self.parent = parent\n",
    "\n",
    "    def add_child(self, child: 'Node'):\n",
    "        self.children.append(child)\n",
    "\n",
    "    def to_dict(self):\n",
    "        \"\"\"Convert the node and its children to a dictionary.\"\"\"\n",
    "        return {\n",
    "            'kind': self.kind,\n",
    "            'code_pos': self.code_pos,\n",
    "            'data': self.data,\n",
    "            'children': [child.to_dict() for child in self.children]\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ecfb533-b0d2-4e32-b1e5-182cb243ed88",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
